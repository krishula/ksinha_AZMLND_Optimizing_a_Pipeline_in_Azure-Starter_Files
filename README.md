# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary

This dataset contains bank marketing campaigns data based on the phone calls to potential clients. The target was to convince the potential clients to make a term deposit at the bank. In this project, we seek to predict whether the potential client would accept to make a term deposit at the bank or not.

The best performing model was Scikit-learn pipeline, which uses logistic regression with hyperdrive to find the best hyperparameters.
Accuracy for Scikit-learn: 0.91685%
Accuracy for AutoML pipeline (VotingEnsemble): 0.91591%

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

**What are the benefits of the parameter sampler you chose?**

**What are the benefits of the early stopping policy you chose?**

Dataset : https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv . 
Data Cleaning and Prep: Fucntion clean_data is used to clean the data of missing values; a data dict is added to convert the categorical fields to numeric; then we one hot encode the data. After that we split the data into using the train_test_split function. 

Tha algorithm used for the training is Logistic Regression. The two hyperparameters of the Logistic Regression are tuned with the hyperdrive to find the model with the best accuracy on the test set. The two hyperparameters are the following:

C: The inverse of the reqularization strength. The smaller the number the stronger the regularization.

max_iter: Maximum number of iterations to converge.

Benefits of the parameter sampler

I chose the RandomParameterSampling, the hyperparameters are randomly selected from the search space. The search space for the two hyperaparameters is the following:

   '--C' : choice(0.001,0.01,0.1,1,10,20,50,100,200,500,1000),
   '--max_iter': choice(50,100,300)
where the choice define discrete space over the values. The benefits of the RandomParameterSampling, is that it is more fast than for example the GridParameterSampling where all the possible values from the search space are used, and it supports early termination of low-performance runs.

Benefits of the early stopping policy

I chose the BanditPolicy which is an "aggressive" early stopping policy with the meaning that cuts more runs than a conservative one like the MedianStoppingPolicy, so it saves computational time. There are three configuration parameters slack_factor, evaluation_interval(optional), delay_evaluation(optional).

slack_factor/slack_amount : (factor)The slack allowed with respect to the best performing training run.(amount) Specifies the allowable slack as an absolute amount, instead of a ratio.

evaluation_interval : (optional) The frequency for applying the policy.

delay_evaluation : (optional) Delays the first policy evaluation for a specified number of intervals.

I set evaluation_interval=2, slack_factor=0.1.



## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
