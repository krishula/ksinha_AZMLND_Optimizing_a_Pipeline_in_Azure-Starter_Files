# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary

This dataset contains bank marketing campaigns data based on the phone calls to potential clients. The target was to convince the potential clients to make a term deposit at the bank. In this project, we seek to predict whether the potential client would accept to make a term deposit at the bank or not.

The best performing model was Scikit-learn pipeline, which uses logistic regression with hyperdrive to find the best hyperparameters.
Accuracy for Scikit-learn: 0.91685%
Accuracy for AutoML pipeline (VotingEnsemble): 0.91591%

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

Dataset : https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv . 
Data Cleaning and Prep: Fucntion clean_data is used to clean the data of missing values; a data dict is added to convert the categorical fields to numeric; then we one hot encode the data. After that we split the data into 85:15 ratio using the train_test_split function. 
Algorithm: Logistic Regression. 
Hyperparameters with their Search Spaces:

1. C: The inverse of the reqularization strength. '--C' : choice(0.001,0.01,0.1,1,10,20,50,100,200,500,1000),
2. max_iter: Maximum number of iterations to converge.  '--max_iter': choice(50,100,300)

**What are the benefits of the parameter sampler you chose?**

RandomParameterSampling is used here instead of GridParameterSampling since, the hyperparameters are randomly selected from the search space vs GridParameterSampling where all the possible values from the search space are used, and it supports early termination of low-performance runs.

**What are the benefits of the early stopping policy you chose?**

BanditPolicy is used here which is an "aggressive" early stopping policy. It cuts more runs than a conservative policy like the MedianStoppingPolicy, hence saving the computational time significantly.
Configuration Parameters:-

1. slack_factor/slack_amount : (factor)The slack allowed with respect to the best performing training run.(amount) Specifies the allowable slack as an absolute amount, instead of a ratio. Set to 0.1.

2. evaluation_interval : (optional) The frequency for applying the policy. Set to 2.

3. delay_evaluation : (optional) Delays the first policy evaluation for a specified number of intervals.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

In AutoML many pipelines are produced simultaneously that run different algorithms and parameters in an automated way. Parameters used to setup AutoML train:

experiment_timeout_minutes = 30

task ='classification' : We'll use "classification" since we seek to predict whether or not the potential client would accept to make a term deposit with the bank.

compute_target : The compute target with specific vm_size and max_nodes.

training_data : The data on which the algorithm will be trained.

label_column_name : The name of the column that contains the labels of the train data.

n_cross_validations=3 : It is how many cross validations to perform when user validation data is not specified.

primary_metric = 'accuracy' : The metric that Automated Machine Learning will optimize for model selection. We have set the 'accuracy'.

enable_early_stopping = True : Whether to enable early termination if the score is not improving in the short term.

The best model was the one with 0.91591% accuracy and the algorithm used was VotingEnsemble. 

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?

Although there isn't much a difference in Accuracy between the two models, AutoML helps us more by showing the importance of each feature for prediction and also shows some useful metric outputs.  

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

In order to improve the Hyperdrive pipeline I would change the early stopping policy to a more conservative policy like the MedianStoppingPolicy, and change the RandomParameterSampling to GridParameterSampling which is more exhaustive computationally. These two changes would make more slow the pipeline because the policy would stop less models and the new sampler would check more parameters.

In order to improve the AutoML I would try to increase the n_cross_validations(for example to 5), change the experiment_timeout_minutes to train longer. I would also change the primary_metric to AUC_weighted because the data are not balanced.


